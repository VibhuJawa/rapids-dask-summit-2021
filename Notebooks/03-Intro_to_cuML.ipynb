{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df964b5-7909-466a-ab66-4b43801e55c8",
   "metadata": {},
   "source": [
    "# Introduction to cuML\n",
    "\n",
    "What is cuML?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba0e05-b26e-45a8-9e72-c8797e14de05",
   "metadata": {},
   "source": [
    "## Introduction and key concepts\n",
    "\n",
    "cuML accelerates machine learning on GPUs. The library follows a couple of key principals, and understanding these will help you take full advantage cuML.\n",
    "\n",
    "### 1. Where possible, match the scikit-learn API\n",
    "\n",
    "cuML estimators look and feel just like scikit-learn estimators. You initialize them with key parameters, fit them with a fit method, then call predict or transform for inference.\n",
    "\n",
    "### 2. Accept flexible input types, return predictable output types\n",
    "\n",
    "cuML estimators can accept NumPy arrays, cuDF dataframes, cuPy arrays, 2d PyTorch tensors, and really any kind of standards-based Python array input you can throw at them.\n",
    "\n",
    "By default, outputs will mirror the data type you provided.\n",
    "\n",
    "### 3. Be fast!\n",
    "\n",
    "On a modern GPU, these can exceed the performance of CPU-based equivalents by a factor of anything from 4x (for a medium-sized linear regression) to over 1000x (for large-scale tSNE dimensionality reduction). In many cases, performance advantages appear as the dataset grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb3bc0-e22d-48e3-bfd8-5326286ecba9",
   "metadata": {},
   "source": [
    "## cuML vs Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b6845-dc8d-41ef-8fbd-0ff671c33987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d7b082-fe3d-4689-8223-6f0f7b798c73",
   "metadata": {},
   "source": [
    "## cuML Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07b1cd-d0b1-4b42-b36d-0f42620a5313",
   "metadata": {},
   "source": [
    "This notebook provides an overview of several machine learning estimators in cuML, demonstrating how to train and evaluate them with built-in metrics functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5cf96-f5b9-4c85-a1c6-5e00450b7227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea29914-c14f-44f0-b3b7-e7c0411cdfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef36e95f-4f5e-4e16-a1ae-e659fcc7dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cuML Estimators\n",
    "\n",
    "### Regressors\n",
    "\n",
    "### Classifiers\n",
    "\n",
    "### Clusterers\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "## cuML Preprocesing\n",
    "\n",
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6231c73-7df9-48d3-89dc-cdb328066b3d",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "To quote the wonderful scikit-learn documentation, `Pipeline` \"sequentially apply a list of transforms and a final estimator\" to a dataset. By collecting transformations and training into a single pipeline, we can confidently do things like cross-validation and hyper-parameter optimization without worrying about data leakage.\n",
    "\n",
    "cuML transformations and estimators are fully compatible with the scikit-learn Pipeline API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfccfa-7402-4a0d-8541-cad9b35764be",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "Model explainability is often critically important. cuML provides a GPU-accelerated SHAP Kernel Explainer and a Permutation Explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ecfea-1015-4560-9eee-2ab6c9b9d31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35afa3-7473-48cb-871b-3ed5ca27cb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "899ab982-b69a-4583-b4e8-2621bb01a484",
   "metadata": {},
   "source": [
    "## Pickling Models\n",
    "\n",
    "So far, we've only stored our models in memory. This final section demonstrates basic pickling of both single-GPU and multi-GPU cuML models for persistence.\n",
    "\n",
    "This is the first use of a multi-GPU model in this notebook. cuML uses Dask for distributed model training. We'll walk through an example below, but we encourage you to explore the cuML documentation for more information and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ae6d5-c54d-48c5-ba77-7162764eaaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838ceee-a829-4b5e-b872-8760150649bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbeabd9-b098-44d1-b944-93cb9668c3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c18af27b-b786-4c7d-9856-5d561332bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.svm import SVC\n",
    "from cuml.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.datasets import make_classification\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea6bbd9b-2376-4f25-b347-e9cb62a881cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFEATURES = 20\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100000,\n",
    "    n_features=NFEATURES,\n",
    "    n_informative=NFEATURES,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    class_sep=0.01,\n",
    "    random_state=0\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a5c59f6-6fd3-4b90-8219-0a95ef57a346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9762799739837646"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba2afbc9-59e1-44ab-97ec-79b85c29ec65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9764000177383423"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49721d6a-945e-46c5-82c3-966893b298c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981440007686615"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"polynomial_feat\", PolynomialFeatures(2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846f869-6ccb-4860-aaf1-b21505b2b5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
